{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90274,"databundleVersionId":10995111,"sourceType":"competition"},{"sourceId":9198133,"sourceType":"datasetVersion","datasetId":5560970},{"sourceId":223071113,"sourceType":"kernelVersion"}],"dockerImageVersionId":30887,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":1995.935386,"end_time":"2025-02-18T03:02:00.966351","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-02-18T02:28:45.030965","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feature Engineering with Fast cuDF-Pandas!\nOne of the most powerful feature engineering techniques is `groupby(COL1)[COL2].agg(STAT)`. This is where we group by `COL1` column and aggregate (i.e. compute) a statistic `STAT` over another column `COL2`. This is the underlying method to compute `target encoding` and `count encoding`. By computing raw statistics and inputting them into our model, our model can do more than only receiving `TE` or `CE`. This notebook illustrates creating 50 engineered features, but we can create hundreds more and improve CV score and LB score!\n\nWhen our dataset has millions of rows like Kaggle's Backpack competition, then `groupby` operations take time to compute. The fastest way to compute a `groupby` aggregation is to use GPU with [RAPIDS cuDF-Pandas][1] library.\n\nThere are two ways to use [RAPIDS cuDF][2]. We can write cuDF code which looks just like Pandas code and starts with `import cudf`. Or we can write normal Pandas code with `import pandas` but before that we add the cell magic command `%load_ext cudf.pandas`. By adding this magic command all calls to Pandas afterward will use [RAPIDS cuDF][2] behind the scenes taking advantage of the massive speed boost of GPU!\n\nAlternatively, we can use [cuDF-Polars][3]. To use [cuDF-Polars][3], we write Polars code with lazy frame. Then the final call includes `.collect(engine=\"gpu\")` which will run all previous Polars code behind the scenes with [RAPIDS cuDF][2].\n\n[1]: https://rapids.ai/cudf-pandas/\n[2]: https://docs.rapids.ai/install/\n[3]: https://rapids.ai/polars-gpu-engine/","metadata":{"papermill":{"duration":0.004363,"end_time":"2025-02-18T02:28:48.234506","exception":false,"start_time":"2025-02-18T02:28:48.230143","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# RAPIDS v25.02\n[RAPIDS v25.02][1] was just released Feb 15, 2025! Instructions on installing RAPIDS is [here][1]. On Kaggle, the easiest way to pip install new libraries is to do it once in a `Utility Script` notebook. Then whenever we attach the `Utility Script` notebook to another Kaggle notebook, the second Kaggle notebook immediately gets the benefit of the pip installed libraries. We created a [RAPIDS 25.02][1] `Utility Script` [here][2], and we attach it to the notebook you are reading. Therefore the notebook you are reading can import RAPIDS v25.02 without needing to pip install!\n\n[1]: https://docs.rapids.ai/install/\n[2]: https://www.kaggle.com/code/cdeotte/rapids-cudf-25-02-cuml-25-02","metadata":{"papermill":{"duration":0.003485,"end_time":"2025-02-18T02:28:48.241835","exception":false,"start_time":"2025-02-18T02:28:48.23835","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# GPU Acceleration\nWe activate [cuDF-Pandas][1] with the magic command `%load_ext cudf.pandas` below. Afterward, all calls to Pandas will use fast GPU [RAPIDS cuDF][2] behind the scenes! Since we attached `Utility Script` notebook [here][3] to the notebook you are reading, we will be using the new [RAPIDS v25.02][2]!\n\n[1]: https://rapids.ai/cudf-pandas/\n[2]: https://docs.rapids.ai/install/\n[3]: https://www.kaggle.com/code/cdeotte/rapids-cudf-25-02-cuml-25-02","metadata":{"papermill":{"duration":0.003342,"end_time":"2025-02-18T02:28:48.248764","exception":false,"start_time":"2025-02-18T02:28:48.245422","status":"completed"},"tags":[]}},{"cell_type":"code","source":"%load_ext cudf.pandas\n\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:23:10.459538Z","iopub.execute_input":"2025-02-25T19:23:10.459870Z","iopub.status.idle":"2025-02-25T19:23:21.867776Z","shell.execute_reply.started":"2025-02-25T19:23:10.459837Z","shell.execute_reply":"2025-02-25T19:23:21.866873Z"},"papermill":{"duration":21.215657,"end_time":"2025-02-18T02:29:09.468801","exception":false,"start_time":"2025-02-18T02:28:48.253144","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Data\nWe load train, train extra, and test data. The combined train data has 4 million rows! This means we do not need to fear overfitting train. We can make hundreds/thousands of new features and every time our CV improves our LB will improve too!","metadata":{"papermill":{"duration":0.003715,"end_time":"2025-02-18T02:29:09.476774","exception":false,"start_time":"2025-02-18T02:29:09.473059","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s5e2/train.csv\")\ntrain_extra = pd.read_csv(\"/kaggle/input/playground-series-s5e2/training_extra.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s5e2/test.csv\")\ntrain = pd.concat([train, train_extra], axis=0, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:23:50.897517Z","iopub.execute_input":"2025-02-25T19:23:50.898127Z","iopub.status.idle":"2025-02-25T19:23:51.352817Z","shell.execute_reply.started":"2025-02-25T19:23:50.898078Z","shell.execute_reply":"2025-02-25T19:23:51.351877Z"},"papermill":{"duration":0.972395,"end_time":"2025-02-18T02:29:10.453077","exception":false,"start_time":"2025-02-18T02:29:09.480682","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original_df = pd.read_csv(\"/kaggle/input/student-bag-price-prediction-dataset/Noisy_Student_Bag_Price_Prediction_Dataset.csv\")\noriginal_df = original_df.groupby(\"Weight Capacity (kg)\").Price.mean()\noriginal_df.name = \"original_Price\"\ntrain = train.merge(original_df, on=\"Weight Capacity (kg)\", how=\"left\")\ntest = test.merge(original_df, on=\"Weight Capacity (kg)\", how=\"left\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:23:56.617009Z","iopub.execute_input":"2025-02-25T19:23:56.617387Z","iopub.status.idle":"2025-02-25T19:23:57.012109Z","shell.execute_reply.started":"2025-02-25T19:23:56.617355Z","shell.execute_reply":"2025-02-25T19:23:57.011111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# merge features from original to train and test df's\noriginal_df = pd.read_csv(\"/kaggle/input/student-bag-price-prediction-dataset/Noisy_Student_Bag_Price_Prediction_Dataset.csv\")\noriginal_df = original_df.loc[(original_df[\"Weight Capacity (kg)\"]>5)&(original_df[\"Weight Capacity (kg)\"]<30)]\noriginal_df.columns = [f\"original_{c}\" for c in original_df.columns]\ntrain = train.merge(original_df.iloc[:,:-1], left_on=\"Weight Capacity (kg)\", right_on=\"original_Weight Capacity (kg)\", how=\"left\")\n#train_df = train_df.drop(\"id\",axis=1)\ntest = test.merge(original_df.iloc[:,:-1], left_on=\"Weight Capacity (kg)\", right_on=\"original_Weight Capacity (kg)\", how=\"left\")\ndisplay(train.info(), test.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:23:59.087213Z","iopub.execute_input":"2025-02-25T19:23:59.087513Z","iopub.status.idle":"2025-02-25T19:23:59.277318Z","shell.execute_reply.started":"2025-02-25T19:23:59.087490Z","shell.execute_reply":"2025-02-25T19:23:59.276453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Engineer Columns\nWe will engineer 8 new columns by combining existing columns.","metadata":{"papermill":{"duration":0.004315,"end_time":"2025-02-18T02:29:12.120568","exception":false,"start_time":"2025-02-18T02:29:12.116253","status":"completed"},"tags":[]}},{"cell_type":"code","source":"CATS = list(train.drop(columns=[\"Price\", \"id\", \"Weight Capacity (kg)\", \"original_Weight Capacity (kg)\"]).columns)\nprint(f\"There are {len(CATS)} categorical columns:\")\nprint( CATS )\nprint(f\"There are 2 numerical columns:\")\nprint( [\"Weight Capacity (kg)\", \"original_Weight Capacity (kg)\"] )","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:24:01.953776Z","iopub.execute_input":"2025-02-25T19:24:01.954089Z","iopub.status.idle":"2025-02-25T19:24:01.967270Z","shell.execute_reply.started":"2025-02-25T19:24:01.954063Z","shell.execute_reply":"2025-02-25T19:24:01.966495Z"},"papermill":{"duration":0.018185,"end_time":"2025-02-18T02:29:12.143122","exception":false,"start_time":"2025-02-18T02:29:12.124937","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"COMBO = []\nfor i,c in enumerate(CATS):\n    #print(f\"{c}, \",end=\"\")\n    combine = pd.concat([train[c],test[c]],axis=0)\n    combine,_ = pd.factorize(combine)\n    train[c] = combine[:len(train)]\n    test[c] = combine[len(train):]\n    n = f\"{c}_wc\"\n    train[n] = train[c]*100 + train[\"original_Weight Capacity (kg)\"]\n    test[n] = test[c]*100 + test[\"original_Weight Capacity (kg)\"]\n    COMBO.append(n)\nfor i,c in enumerate(CATS):\n    #print(f\"{c}, \",end=\"\")\n    combine = pd.concat([train[c],test[c]],axis=0)\n    combine,_ = pd.factorize(combine)\n    train[c] = combine[:len(train)]\n    test[c] = combine[len(train):]\n    n = f\"{c}_orig_wc\"\n    train[n] = train[c]*100 + train[\"original_Weight Capacity (kg)\"]\n    test[n] = test[c]*100 + test[\"original_Weight Capacity (kg)\"]\n    COMBO.append(n)\nprint()\nprint(f\"We engineer {len(COMBO)} new columns!\")\nprint( COMBO )","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:24:16.601188Z","iopub.execute_input":"2025-02-25T19:24:16.601493Z","iopub.status.idle":"2025-02-25T19:24:18.352946Z","shell.execute_reply.started":"2025-02-25T19:24:16.601471Z","shell.execute_reply":"2025-02-25T19:24:18.352202Z"},"papermill":{"duration":0.857441,"end_time":"2025-02-18T02:29:13.005106","exception":false,"start_time":"2025-02-18T02:29:12.147665","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURES = CATS + [\"Weight Capacity (kg)\", \"original_Weight Capacity (kg)\"] + COMBO\nprint(f\"We now have {len(FEATURES)} columns:\")\nprint( FEATURES )\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:24:20.806807Z","iopub.execute_input":"2025-02-25T19:24:20.807165Z","iopub.status.idle":"2025-02-25T19:24:20.838158Z","shell.execute_reply.started":"2025-02-25T19:24:20.807137Z","shell.execute_reply":"2025-02-25T19:24:20.837447Z"},"papermill":{"duration":0.015669,"end_time":"2025-02-18T02:29:13.030197","exception":false,"start_time":"2025-02-18T02:29:13.014528","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_cols = train.select_dtypes(include=['float64']).columns\ntrain[train_cols] = train[train_cols].astype('float32')\ntrain_cols = train.select_dtypes(include=['int64']).columns\ntrain[train_cols] = train[train_cols].astype('int16')\n\ntest_cols = test.select_dtypes(include=['float64']).columns\ntest[test_cols] = test[test_cols].astype('float32')\ntest_cols = test.select_dtypes(include=['int64']).columns\ntest[test_cols] = test[test_cols].astype('int16')\ntrain.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:24:23.982980Z","iopub.execute_input":"2025-02-25T19:24:23.983340Z","iopub.status.idle":"2025-02-25T19:24:34.133141Z","shell.execute_reply.started":"2025-02-25T19:24:23.983308Z","shell.execute_reply":"2025-02-25T19:24:34.132277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGBoost with Feature Engineer GroupBy\nWe train XGBoost with nested folds. We use the inner nested fold to create new features that aggregate the target `price`. And we use the outer fold to create new features that do not aggregate the target `price`. In each k fold loop, we engineer new features using the advanced feature engineering technique `groupby(COL1)[COL2].agg(STAT)`. Since we are using [RAPIDS cuDF-Pandas][1], these groupby computations will run fast on GPU! And we will train our model quickly on GPU using XGBoost!\n\n[1]: https://rapids.ai/cudf-pandas/","metadata":{"papermill":{"duration":0.006054,"end_time":"2025-02-18T02:29:13.044966","exception":false,"start_time":"2025-02-18T02:29:13.038912","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nimport cudf\nimport cupy as cp\nprint(f\"XGBoost version\",xgb.__version__)","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:24:37.442124Z","iopub.execute_input":"2025-02-25T19:24:37.442450Z","iopub.status.idle":"2025-02-25T19:24:39.471383Z","shell.execute_reply.started":"2025-02-25T19:24:37.442424Z","shell.execute_reply":"2025-02-25T19:24:39.470449Z"},"papermill":{"duration":3.600119,"end_time":"2025-02-18T02:29:16.649658","exception":false,"start_time":"2025-02-18T02:29:13.049539","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STATISTICS TO AGGEGATE FOR OUR FEATURE GROUPS\nSTATS = [\"mean\",\"std\",\"count\",\"median\",\"min\",\"max\"]\nSTATS2 = [\"mean\",\"std\"]","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:24:42.168433Z","iopub.execute_input":"2025-02-25T19:24:42.169032Z","iopub.status.idle":"2025-02-25T19:24:42.172785Z","shell.execute_reply.started":"2025-02-25T19:24:42.168999Z","shell.execute_reply":"2025-02-25T19:24:42.171864Z"},"papermill":{"duration":0.010236,"end_time":"2025-02-18T02:29:16.665371","exception":false,"start_time":"2025-02-18T02:29:16.655135","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optimize_dtypes(df):\n    \"\"\"Convert float64→float16 and int64→int16 to reduce memory usage.\"\"\"\n    for col in df.select_dtypes(include=['float64']).columns:\n        df[col] = df[col].astype('float32')\n    for col in df.select_dtypes(include=['int64']).columns:\n        df[col] = df[col].astype('int16')\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T19:24:46.520234Z","iopub.execute_input":"2025-02-25T19:24:46.520573Z","iopub.status.idle":"2025-02-25T19:24:46.525461Z","shell.execute_reply.started":"2025-02-25T19:24:46.520543Z","shell.execute_reply":"2025-02-25T19:24:46.524379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FOLDS = 5\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\noof = np.zeros(len(train))\npred = np.zeros(len(test))\n\n# OUTER K-FOLD\nfor i, (train_index, test_index) in enumerate(kf.split(train)):\n    print(f\"### OUTER Fold {i+1} ###\")\n\n    X_train = train.loc[train_index, FEATURES + ['Price']].reset_index(drop=True).copy()\n    y_train = train.loc[train_index, 'Price']\n\n    X_valid = train.loc[test_index, FEATURES].reset_index(drop=True).copy()\n    y_valid = train.loc[test_index, 'Price']\n\n    X_test = test[FEATURES].reset_index(drop=True).copy()\n\n    # INNER K-FOLD (TO PREVENT LEAKAGE WHEN USING PRICE)\n    kf2 = KFold(n_splits=FOLDS, shuffle=True, random_state=42)   \n    for j, (train_index2, test_index2) in enumerate(kf2.split(X_train)):\n        print(f\" ## INNER Fold {j+1} (Outer Fold {i+1}) ##\")\n\n        X_train2 = X_train.loc[train_index2, FEATURES + ['Price']].copy()\n        X_valid2 = X_train.loc[test_index2, FEATURES].copy()\n\n        ### FEATURE SET 1 (Uses Price) ###\n        col = \"Weight Capacity (kg)\"\n        tmp = X_train2.groupby(col).Price.agg(STATS)\n        tmp.columns = [f\"TE1_wc_{s}\" for s in STATS]\n        X_valid2 = X_valid2.merge(tmp, on=col, how=\"left\")\n        for c in tmp.columns:\n            X_train.loc[test_index2, c] = X_valid2[c].values\n\n        col = \"original_Weight Capacity (kg)\"\n        tmp = X_train2.groupby(col).Price.agg(STATS)\n        tmp.columns = [f\"TE1_orig_wc_{s}\" for s in STATS]\n        X_valid2 = X_valid2.merge(tmp, on=col, how=\"left\")\n        for c in tmp.columns:\n            X_train.loc[test_index2, c] = X_valid2[c].values\n\n        ### FEATURE SET 2 (Uses Price) ###\n        for col in COMBO:\n            tmp = X_train2.groupby(col).Price.agg(STATS2)\n            tmp.columns = [f\"TE2_{col}_{s}\" for s in STATS2]\n            X_valid2 = X_valid2.merge(tmp, on=col, how=\"left\")\n            for c in tmp.columns:\n                X_train.loc[test_index2, c] = X_valid2[c].values\n\n    ### FEATURE SET 1 (Uses Price) ###\n    col = \"Weight Capacity (kg)\"\n    tmp = X_train.groupby(col).Price.agg(STATS)\n    tmp.columns = [f\"TE1_wc_{s}\" for s in STATS]\n    X_valid = X_valid.merge(tmp, on=col, how=\"left\")\n    X_test = X_test.merge(tmp, on=col, how=\"left\")\n\n    col = \"original_Weight Capacity (kg)\"\n    tmp = X_train.groupby(col).Price.agg(STATS)\n    tmp.columns = [f\"TE1_orig_wc_{s}\" for s in STATS]\n    X_valid = X_valid.merge(tmp, on=col, how=\"left\")\n    X_test = X_test.merge(tmp, on=col, how=\"left\")\n\n    ### FEATURE SET 2 (Uses Price) ###\n    for col in COMBO:\n        tmp = X_train.groupby(col).Price.agg(STATS2)\n        tmp.columns = [f\"TE2_{col}_{s}\" for s in STATS2]\n        X_valid = X_valid.merge(tmp, on=col, how=\"left\")\n        X_test = X_test.merge(tmp, on=col, how=\"left\")\n\n    # Convert newly created columns to optimized dtypes\n    X_train = optimize_dtypes(X_train)\n    X_valid = optimize_dtypes(X_valid)\n    X_test = optimize_dtypes(X_test)\n\n    # CONVERT TO CATS SO XGBOOST RECOGNIZES THEM\n    X_train[CATS] = X_train[CATS].astype(\"category\")\n    X_valid[CATS] = X_valid[CATS].astype(\"category\")\n    X_test[CATS] = X_test[CATS].astype(\"category\")\n\n    # DROP PRICE THAT WAS USED FOR TARGET ENCODING\n    X_train = X_train.drop(['Price'], axis=1)\n\n    # Convert to CuPy (for GPU acceleration)\n    X_train = cp.asarray(X_train)\n    X_valid = cp.asarray(X_valid)\n    X_test = cp.asarray(X_test)\n\n    y_train = cp.asarray(y_train)\n    y_valid = cp.asarray(y_valid)\n\n    # Convert to XGBoost DMatrix (GPU enabled)\n    dtrain = xgb.DMatrix(X_train, label=y_train, nthread=-1)\n    dvalid = xgb.DMatrix(X_valid, label=y_valid, nthread=-1)\n    dtest = xgb.DMatrix(X_test, nthread=-1)\n\n    # Set XGBoost parameters (GPU enabled)\n    params = {\n        \"max_depth\": 6,\n        \"colsample_bytree\": 0.5,\n        \"subsample\": 0.5,\n        \"learning_rate\": 0.02,\n        \"min_child_weight\": 10,\n        \"tree_method\": \"hist\",\n        \"device\":\"cuda\"\n    }\n    \n    # Train the model\n    evallist = [(dtrain, \"train\"), (dvalid, \"valid\")]\n    model = xgb.train(\n        params,\n        dtrain,\n        num_boost_round=10_000,  # Instead of n_estimators\n        evals=evallist,\n        early_stopping_rounds=100,\n        verbose_eval=300\n    )\n\n\n    # Predict OOF and Test using DMatrix\n    oof[test_index] = model.predict(dvalid)\n    pred += model.predict(dtest)\n\npred /= FOLDS","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:24:52.334199Z","iopub.execute_input":"2025-02-25T19:24:52.334579Z","iopub.status.idle":"2025-02-25T19:41:46.395557Z","shell.execute_reply.started":"2025-02-25T19:24:52.334540Z","shell.execute_reply":"2025-02-25T19:41:46.394560Z"},"papermill":{"duration":1955.146651,"end_time":"2025-02-18T03:01:51.816981","exception":false,"start_time":"2025-02-18T02:29:16.67033","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Overall CV Score\nBelow we display overall cv score and save oof predictions to disk so we can use them later to assist finding ensemble weights with our other models.","metadata":{"papermill":{"duration":0.010703,"end_time":"2025-02-18T03:01:51.839031","exception":false,"start_time":"2025-02-18T03:01:51.828328","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# COMPUTE OVERALL CV SCORE\ntrue = train.Price.values\ns = np.sqrt(np.mean( (oof-true)**2.0 ) )\nprint(f\"=> Overall CV Score = {s}\")","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:42:01.691772Z","iopub.execute_input":"2025-02-25T19:42:01.692119Z","iopub.status.idle":"2025-02-25T19:42:03.845974Z","shell.execute_reply.started":"2025-02-25T19:42:01.692073Z","shell.execute_reply":"2025-02-25T19:42:03.845218Z"},"papermill":{"duration":0.78111,"end_time":"2025-02-18T03:01:52.630915","exception":false,"start_time":"2025-02-18T03:01:51.849805","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SAVE OOF TO DISK FOR ENSEMBLES\nVER = 1\nnp.save(f\"oof_v{VER}\",oof)\nprint(\"Saved oof to disk\")","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:42:16.832328Z","iopub.execute_input":"2025-02-25T19:42:16.832665Z","iopub.status.idle":"2025-02-25T19:42:16.859718Z","shell.execute_reply.started":"2025-02-25T19:42:16.832635Z","shell.execute_reply":"2025-02-25T19:42:16.858812Z"},"papermill":{"duration":0.039041,"end_time":"2025-02-18T03:01:52.681063","exception":false,"start_time":"2025-02-18T03:01:52.642022","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Names\nBelow we list all our engineered features. We are using 57 features in total!","metadata":{"papermill":{"duration":0.010308,"end_time":"2025-02-18T03:01:52.702663","exception":false,"start_time":"2025-02-18T03:01:52.692355","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(f\"\\nIn total, we used {dtrain.num_col()} features, Wow!\\n\")","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:49:26.582422Z","iopub.execute_input":"2025-02-25T19:49:26.582761Z","iopub.status.idle":"2025-02-25T19:49:26.588082Z","shell.execute_reply.started":"2025-02-25T19:49:26.582735Z","shell.execute_reply":"2025-02-25T19:49:26.587114Z"},"papermill":{"duration":0.063025,"end_time":"2025-02-18T03:01:52.77659","exception":false,"start_time":"2025-02-18T03:01:52.713565","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# XGB Feature Importance\nHere is XGBoost feature importance sorted by `gain`.","metadata":{"papermill":{"duration":0.009924,"end_time":"2025-02-18T03:01:52.797167","exception":false,"start_time":"2025-02-18T03:01:52.787243","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import xgboost as xgb\nfig, ax = plt.subplots(figsize=(10, 20))\nxgb.plot_importance(model, importance_type='gain',ax=ax)\nplt.title(\"Top Feature Importances (XGBoost)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:49:48.696352Z","iopub.execute_input":"2025-02-25T19:49:48.696677Z","iopub.status.idle":"2025-02-25T19:49:50.056024Z","shell.execute_reply.started":"2025-02-25T19:49:48.696652Z","shell.execute_reply":"2025-02-25T19:49:50.055185Z"},"papermill":{"duration":0.709464,"end_time":"2025-02-18T03:01:53.517477","exception":false,"start_time":"2025-02-18T03:01:52.808013","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Make Submission CSV\nWe save our test predictions to submission.csv and plot our predictions. ","metadata":{"papermill":{"duration":0.014783,"end_time":"2025-02-18T03:01:53.547748","exception":false,"start_time":"2025-02-18T03:01:53.532965","status":"completed"},"tags":[]}},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/playground-series-s5e2/sample_submission.csv\")\nsub.Price = pred\nsub.to_csv(f\"submission_v{VER}.csv\",index=False)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:50:02.343888Z","iopub.execute_input":"2025-02-25T19:50:02.344256Z","iopub.status.idle":"2025-02-25T19:50:02.528294Z","shell.execute_reply.started":"2025-02-25T19:50:02.344224Z","shell.execute_reply":"2025-02-25T19:50:02.527405Z"},"papermill":{"duration":0.096407,"end_time":"2025-02-18T03:01:53.658057","exception":false,"start_time":"2025-02-18T03:01:53.56165","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nplt.hist(sub.Price,bins=100)\nplt.title(\"Test Predictions\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-02-25T19:50:07.258023Z","iopub.execute_input":"2025-02-25T19:50:07.258382Z","iopub.status.idle":"2025-02-25T19:50:13.535837Z","shell.execute_reply.started":"2025-02-25T19:50:07.258353Z","shell.execute_reply":"2025-02-25T19:50:13.535007Z"},"papermill":{"duration":3.719347,"end_time":"2025-02-18T03:01:57.393096","exception":false,"start_time":"2025-02-18T03:01:53.673749","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}